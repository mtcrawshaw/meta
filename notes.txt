Road map:
    PPO for single task
    Add MetaWorld support
--> Better evaluation
    Splitting networks for MTL
    PCGrad, sparse sharing, soft-layer ordering, other baselines for MTL
    Splitting networks for meta learning
    MAML and other baselines for meta learning
    Hyperparameter optimization
    Tests of statistical significance
    Learned loss convexity regularization for MTL

Current sprint:
--> EMA of rewards instead of looking at past N
    Evaluate success rate during training
    Saving settings, metrics under results
    Option to plot mean + std dev
    Move legend outside of axes
    Evaluate success rate after training

Before merging to master:
- Use black, pylint, and mypy to clean up
- Make sure "pytest meta/" passes

Future additions:
- Test PPO for environments with action space Box((n1, n2))
- PPO training settings to add/consider:
    - Network architecture hyperparameters (more customized architectures)
    - Choice of optimizer and optimizer hyperparameters
    - Parameter sharing between actor and critic
    - Deterministic acting (i.e. always choose action with highest prob)
- Make tests deterministic
- Move tests outside of package
- Add feature so that task-index part of observation is not normalized

Possible additions:
- Optimize data transfer to GPU
- Different ways to collect data from multiple tasks. Cycle through tasks, randomly pick
  one, randomly pick one that is different from previous.
- Refactor RolloutStorage data generators. Put tensors into a dict. Can we just use good
  indexing to construct the batches instead of looping and stacking?
- Refactor naming in PPO implementation to clarify between episodes, rollouts, and
  trajectories

----------------------------------------------------------------------------------------

Tomorrow:
Write tests for Metrics, and add EMA metrics to original_benchmark so we can still
compare if we need to. Right now tests are failing, all of the MT10 multi-processed
results come back different than the original. Maybe this is because the different
processes are finishing in different orders on various runs of the test cases and this
causes the computation of the metric to be different? Not sure.
