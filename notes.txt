Todo:
Add PPO for single task
- Just get PPO running on CPU
    - Use not tiny batches
    - Beat CartPole-v1
    - Make sure PolicyNetwork.logstd is changing
    - Write tests
      - Loss values/intermediate calculations
      - Tensor sizes
- Clean up
    - Build up rollouts as numpy arrays
    - Make tensor shapes consistent
    - Move to settings file
    - Convert to package
    - black/pylint/mypy
- Add GPU support
- Add multiprocessing
- Add support for recurrent policies?
Add MAML for single task distribution
Add modular meta learning for meta learning
Other baselines for meta learning?
Implement new ideas for meta learning
Implement ML^3 for multi task learning
Implement learned loss convexity regularization for multi task learning

Training settings to add/consider:
- Network architecture hyperparameters
- Choice of optimizer and optimizer hyperparameters
- Parameter sharing between actor and critic
- Collecting data from multiple episodes into a single (larger) minibatch

Leads for PPO bugs:
- Manual tests
- Why does action_loss seem to be backward? The value is small when the policy
  is randomly initialized, then the policy will converge to around 9.5 average
  reward (which means the agent is dying as quickly as possible) and then the
  value of action_loss peaks and plateaus. Why does it increase there?
- Tensor shapes weren't consistent before, caused some weird errors with
  broadcasting results when shapes didn't match up. Inspect rollout information
  again and make sure everything is still good, especially log probs
  calculations.
- It seems to work when value_loss_coeff = 0.0. WTF
