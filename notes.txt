Todo:
Add PPO for single task
- Just get PPO running on CPU
    - Write tests
      - Vary environments
      - Make tests deterministic given settings
      - Introduce hypothesis
    - Beat single tasks from MetaWorld
    - Make sure PolicyNetwork.logstd is changing
- Clean up
    - Build up rollouts as numpy arrays
    - Make tensor shapes consistent
    - Move to settings file
    - Convert to package
    - black/pylint/mypy
- Add GPU support
- Add multiprocessing
- Add support for recurrent policies?
Implement multitask PPO for multi task learning
Implement PCGrad for multi task learning
Implement new idea for multi task learning
Implement ML^3 for multi task learning
Implement learned loss convexity regularization for multi task learning
Implement MAML for meta learning single task distribution
Add modular meta learning for meta learning
Other baselines for meta learning?
Implement new ideas for meta learning

PPO training settings to add/consider:
- Network architecture hyperparameters
- Choice of optimizer and optimizer hyperparameters
- Parameter sharing between actor and critic
- Collecting data from multiple episodes into a single (larger) minibatch

Leads for PPO bugs:
- Why does action_loss seem to be backward? The value is small when the policy is
  randomly initialized, then the policy will converge to around 9.5 average reward
  (which means the agent is dying as quickly as possible) and then the value of
  action_loss peaks and plateaus. Why does it increase there?
- Tensor shapes weren't consistent before, caused some weird errors with broadcasting
  results when shapes didn't match up. Inspect rollout information again and make sure
  everything is still good, especially log probs calculations.
- It seems to work when value_loss_coeff = 0.0. This is especially weird because the
  parameters of the actor are not at all involved in the computation of value_loss. If
  you set action_loss to just value_loss and then look at the values of param.grad for
  name, param in self.policy_network.named_parameters(), the gradients of the actor
  parameters are None. (Note: After combining rollout info from multiple episodes into
  one batch, training no longer works even when value_loss_coeff = 0.0)
- Found the bug! In commit 79fc, returns is instantiated to have size
  rollouts.rollout_length, when it should really be rollouts.rollout_step. Unfortunately
  this typo doesn't exist in the current HEAD (commit 0dc9) so that means there is
  another bug after adding handling of multiple episodes per update. Time to write
  tests!
- Curiously, the test for the loss calculation (now called
  test_ppo.test_update.values()) fails when the number of policy layers is greater than
  1. The difference between the expected and the actual values seem to grow as the
  number of layers grows.
- If you run ``pytest ./`` and look at the error output for test_update_values(), you
  can see that the returned action loss is always nearly zero. Is that correct?
- Fixed bugs in loss computation in test_ppo.py, tests now all pass. Reward still
  doesn't go up for CartPole-v1.

Current tasks:
- Add reverted features back in
- Control randomness in training with seed
- Fix occasional invalid multinomial distribution error
