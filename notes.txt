Todo:
Add PPO for single task
- Just get PPO running on CPU
    - Handle dones
    - Try it on a small easy environment
    - Make sure PolicyNetwork.logstd is changing
    - Make tensor shapes consistent
    - Find out if retain_graph=True is okay
    - Build up rollouts as numpy arrays
    - Write tests
- Clean up
    - Move to settings file
    - Convert to package
    - black/pylint/mypy
- Add GPU support
- Add multiprocessing
- Add support for recurrent policies
Add MAML for single task distribution
Add modular meta learning for meta learning
Other baselines for meta learning?
Implement new ideas for meta learning
Implement ML^3 for multi task learning
Implement learned loss convexity regularization for multi task learning

Training settings to add/consider:
- Network architecture hyperparameters
- Choice of optimizer and optimizer hyperparameters
- Parameter sharing between actor and critic
- Gradient clipping
- Value loss clipping
- Whether or not to add value_pred to advantageg estimate

Leads for PPO bugs:
- retain_graph=True
- Super small batches?
- Write manual test
