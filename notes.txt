Road Map


Major versions:
    PPO for single task
    Multi-task/MetaWorld support
    Training tools
--> Splitting networks for RL
    Computer vision support
    Sparse splitting networks
    NLP support

Minor versions (MT10/MT50 baselines):
    Shared trunk architecture
    FC splitting networks (regions = layers)
    Meta splitting networks
--> Baselines (
        PCGrad,
        Branched MT networks,
        soft layer ordering,
        modular meta learning (code available),
        AdaShare,
        Piggyback (code available),
        sparse sharing (code available),
        routing networks (code available),
    )
    Refactoring

Current sprint (Meta splitting networks):
--> Update Meta-World
    Add option to perform single-task training on Meta-World without resampling goals
    Benchmark performance for single-task learning
    Benchmark performance for multi-task learning with splitting networks and multi-task PPO
    Benchmark performance for meta-learning with splitting networks
    Improve performance for single-task learning
    Improve performance for multi-task learning with splitting networks
    Improve performance for meta-learning with splitting networks
    Baselines?

Notes for current task:
- Confirm assumptions about format of info passed from env step() (DONE)
- Check that task indices (when setting effective task index) haven't changed (DONE)
- Change time limits from 150 to 200 (DONE)
- Check config files for other changes (DONE)
  - normalize_first_n (DONE)
  - time_limit (DONE)
- New tests:
  - Training/meta-training for each metaworld benchmark (DONE)
  - Goals are resampled/fixed correctly, initial observations are resampled/fixed
    correctly, and one-hot vectors are included in observations correctly (DONE)
- Reduce memory cost of metaworld benchmarks by only instantiating a single environment
  at a time

Performance improvements:
- Use SAC instead of PPO
- Combine splitting networks with PCGrad
- Hyperparameter tuning

For benchmarking performance, make sure that metrics are being saved as moving averages
and not exponential moving averages. In the single-task experiments so far, the eval
reward/success rate in the plots is always smaller than that of training. Could this be
because the eval stats are being computed with a more slowly updating average than the
train stats?


----------------------------------------------------------------------------------------


Practices


Before merging to master:
- Use black, pylint, and mypy to clean up
- Make sure "pytest tests/" passes


----------------------------------------------------------------------------------------


Future Plans


DevOps:
- Make sure that a clean install works properly (repo contains all data necessary for
  tests, requirements contains all requirements, etc)
- Move test files (configs, results, metrics, etc.) into a separate subdirectory
- Containerize this shit
- Update MetaWorld version and make corresponding changes

Training infrastructure:
- Refactoring:
    - Move optimizer and learning schedule outside of ppo.py in order to generalize
      training to CV and NLP.
    - Optimize data transfer to GPU
    - Use Metrics objects outside of train() instead of state dictionary.
    - Get rid of redundant functionality for saving metrics
    - Move metric comparison outside of train() and hyperparameter_search(). We wouldn't
      have to do so much bullshit to save/load/compare metrics if the functionality was
      modular
    - Fix learning rate schedule behavior when resuming runs. Right now, if a run
      finishes, and then it is resumed and trained for a longer period of time, the
      learning rate will not align with the original schedule. Really this could be
      fixed by creating two different loading modes: resume and extend. Resume will
      follow the original schedule for parameters, and extend will ditch the old one and
      follow the newly given one.
    - Fix cosine learning rate schedule definition (T_max should be num_updates - 1) and
      create new corresponding baselines
    - Create config/architecture_config object so we don't have to mess with
      string-keyed dictionaries everywhere
    - Change metrics measuring to be more like logging. Then we can just add anything to
      metrics from anywhere. The metrics object should just accept a Dict[str,
      List[float]], where keys are metric names and values are new metric values to add
      to that metric's history. If the string is new, then we just start a history for a
      new metric. This will be a lot more flexible than hand-coding in each individual
      metric.
    - Convert string literals in config (network architecture type, lr schedule type,
      environment names, etc) into enums.
    - Fix reproducibility issue with meta RL. At the moment, the goals sampled in the
      Meta-World environments are inconsistent between training runs, but only during
      meta-testing. Right now, we are only testing that the baselines line up for
      meta-training, which I actually think is fine for now since meta-testing is
      essentially just an evaluation, and is much shorter in practice than
      meta-training. The goals are sampled from using np.ndarray.sample(), and we do set
      the np seeds at the beginning of both meta-training and meta-testing. The fact
      that this is only happening during meta-training makes me feel like this is an
      issue with setting the seed twice, but I don't see how that could be true. It may
      also be specific to the Meta-World environments somehow.
- Additions:
    - Training options:
        - Choice of optimizer and optimizer hyperparameters
        - Learning rate schedules (ReduceLROnPlateau, cyclic LR)
    - Multi-task architecture support for environments with observations whose
      dimension is greater than 1
    - Add tests checking saved/loaded values for saving/loading training runs. This
      can't be done right now since the training function is so monolithic, we can't
      "look inside" and check the values of the local variabes of that function. Once
      this functionality is more modular we should add these tests. For now I just
      manually inspected the values after loading and created a baseline to compare
      metrics against. The same goes for saving/loading hyperparameter random search
      runs (the other types of hp search are reproducible when saving/loading, so
      there's no need for it there).
    - Tests of statistical significance of results when comparing different methods
    - Allow for training on a sequence of configs by specifying a list of values in the
      config file instead of a single value. Just getting training to work like this
      will be easy, the more annoying part will be saving and loading over these
      compound runs.
    - Measure training metrics on each individual task during multi-task training.
    - Saving/loading for meta-learning runs.
    - Add option to meta-test by loading a meta-trained model.
- Nice to have:
    - Compute standard deviation for success rate so that mean + stdev < 1 i.e. makes
      sense for a variable in the range [0, 1]. Easy way to do this is to assume that
      tan^(-1)(X) is normally distributed instead of X, compute mean + stdev and mean -
      stdev, then take tan of everything to put it back into interval [0, 1].
    - Time various parts of the training process, save along with metrics.
    - More robust way to save repository version along with results.
    - Catch KeyboardInterrupt during training to optionally stop training early while
      still saving out final metrics and plot.
    - When training starts and results are not being saved, notify user and give the
      option to turn saving on.
    - Make it so that when resuming training from a checkpoint, training happens exactly
      as it would had if training had never been stopped. Not sure if this is possible,
      the main obstacle being pickling/loading the environment state, and mimicking
      random decisions.
    - Monitor system resource utilization and recommend changes in config for maximum
      utilization, i.e. bigger batch sizes to increase GPU utilization, less processes
      if thrashing is present.
    - Output hyperparameter tables in performance plots.

RL training:
- Refactoring:
    - Refactor RolloutStorage data generators. Put tensors into a dict. Can we just use
      good indexing to construct the batches instead of looping and stacking?
    - Refactor naming in PPO implementation to clarify between episodes, rollouts, and
      trajectories
    - Rename PPO parameters (i.e. gamma -> discount)
    - Figure out the difference between our setup and other setups (such as multi-task
      PPO in the Meta-World paper) which use a linear feature baseline.
- Additions:
    - Test PPO for environments with action space Box((n1, n2))
    - Independent options for normalizing observations and rewards
    - Network architecture hyperparameters:
        - Parameter sharing between actor and critic (and separate recurrent layers)
        - Recurrent layers throughout network, different types of recurrent layers
        - Value function normalization
    - Options to collect data from multiple tasks. Cycle through tasks, randomly pick
      next, randomly pick next that is different from previous. One option should ensure
      that multi-process training is collecting data from tasks as uniformly as
      possible.
- Nice to have:
    - Rendering of episode videos
    - Determine whether or not policy has solved environment (N successes in a row,
      usually)
    - Deparallelization factor as an option. We include an option called
      `rollouts_per_process` that dictates how many rollouts each process should execute
      during a training step. Ideally, running 4 processes with 1 rollout per process
      would be the same as running 2 processes with 2 rollouts per process, but having
      the option to choose allows us to maximize the efficiency of whatever hardware we
      are running on. On my laptop, the limiting factor is RAM, so decreasing
      parallelization (i.e. increasing rollouts_per_process) is the way to go to avoid
      thrashing. The reason I haven't implemented this yet is because there isn't really
      a good answer to the following: should we reset the environment between
      consecutive rollouts for the same process? If the answer is yes, then we will
      never get past the first `rollout_length` steps of an episode, but if the answer
      is no, then running multiple rollouts per process is effectively the same as just
      increasing the rollout length. What we really want is to be able to reset the
      environment between rollouts from the same process, then restore the environment
      state of each rollout after the training step. However, this requires storing the
      environment state (don't wanna do that) and would greatly increase memory cost,
      which is the whole reason we want a deparallelization option in the first place.
      The increase in memory wouldn't actually be that bad, because even though the
      amount of memory needed for each process would increase by a factor of
      `rollouts_per_process`, that memory would be partitioned into
      `rollouts_per_process` pieces which each only need to exist in RAM one at a time,
      so I don't think we would see thrashing here. Still, storing and restoring the
      environment state is a pain and for now I'm just gonna increase the rollout
      length.

Network architectures:
- Refactoring:
    - Remove redundant calls to "self.to(device)" in nested modules?
- Additions:
    - Architecture hyperparameters:
        - Varying network size in hidden layers
        - Network initialization options
        - ReLU + other activation functions
    - An option to turn on checks during inference (safe mode). We would check that each
      observation in a multi-task setting ends with a one-hot tensor of size
      `self.num_tasks`, and that task gradients from get_task_grads() from
      MultiTaskSplittingNetwork have the same zero padding structure before finding
      their difference, etc. This way we still have the option to check and be safe, but
      we don't have to waste computation time doing so if we don't want to. This also
      applies to checks of tensor structure in forward() of MultiTaskTrunkNetwork,
      get_loss() from PPOPolicy, and probably a bunch of other places.
- Nice to have:

Splitting networks:
- Refactoring:
    - Refactor MultiTaskSplittingNetwork.get_task_grad_diffs() to only compute pairwise
      differences between task gradients for tasks that share a copy. Right now it is
      computed for each pair of tasks at each region. We could also parallelize the
      computation across regions but I'm not quite sure how to do that.
    - In splitting networks, modify task gradient computation so that we don't perform a
      redundant backwards pass. Right now, we pass over each task loss, then zero
      everything out and pass over the sum of losses. We don't actually need to do that
      last backward pass, but we need to do some stuff to get the task losses without
      zeroing out gradients between each backward pass. Really we just need to take the
      successive differences of gradients, and we also need to change the format of the
      backward pass in general so that .backward() does get called on the entire loss if
      we aren't using splitting networks. Also, the problem is twice as bad as we
      originally thought: we are calling backwards on each task loss twice! Once in
      check_for_splits() for the actor and again for the critic. Fixing this
      inefficiency is gonna be messy.
    - Refactor MultiTaskSplittingNetwork.update_grad_stats() to get task flags in a way
      that is less brittle than checking whether the gradient vector is all zero. Maybe
      this is an edge case that we actually want to account for. We could probably get
      this info from `task_losses`.
- Additions:
    - Option to reset splitting statistics after a split is performed (this means we
      have to split regions with highest z-scores first)
    - Try training splitting networks by measuring gradient variance online, just
      with a smaller EMA alpha value. It might be that an EMA which moves too slowly
      will yield a variance which is larger than it should be, making the test for
      splitting not sensitive enough.
    - Support the inclusion of task index in input to splitting/meta-splitting networks.
      This is pretty messy, since we need to distinguish tasks from meta-training
      distribution and tasks from meta-testing distribution, but we also need to use the
      same layers between meta-training nad meta-testing. We can't just include the task
      index, because there may not be the same number of training/testing tasks. Even if
      there are, the network would think that an observation from testing task 1
      actually came from training task 1. If we try to use one-hot vectors whose size is
      the total number of training+testing tasks, then we end up in a situation where
      the weights used to transform the testing task inputs never get trained during
      meta-training, and then get frozen for meta-testing. The only solution I can think
      to this is to unfreeze specificially those weights during meta-testing, but this
      is not super clean and also goes against the spirit of modularity in splitting
      networks? Not sure.
- Nice to have:
    - In actor/critic network, add splitting for action_logstd. Right now each task just
      has a separate set of these parameters.
    - Allow weights/biases to be split independently

Hyperparameter search:
- Refactoring:
    - Get rid of redundant settings inside and outside base_train_config for
      hyperparameter search, i.e. seed, save name, etc
    - Get rid of repeated code in different search strategies
    - Clean up redundant options between "base_train_config" and "hp_config"
    - Change hyperparameter search to allow non-unique leaf node names in config. After
      allowing separate architecture configs for actor and critic networks, we have a
      bunch of parameters with non-unique names. What this means is that parameters with
      the same name will be tied together during the search process. To fix this, we
      need to change the way that we specify search spaces for parameters. Right now
      each search space is tied to a parameter just by its name, which is why we needed
      unique names, but what we really need is for each search space to be tied to the
      path through the config tree to the corresponding leaf. To do this, we should just
      specify the search spaces within the base train config dictionary. Instead of
      having individual values within this config, we just have a list of values, or
      another dict which describes the search space.
- Additions:
    - Another hp search strategy: IC grid search, but which parameter to vary is chosen
      automatically. Each parameter is varied once to get an initial tuning for each
      one, then the next parameter to tune is sampled from a distribution in which the
      probability that a parameter gets chosen is equal to some normalized estimate of
      the potential gain from tuning that parameter. A natural choice is to use the
      standard deviation of the rewards from the previous tuning as this estimate. Once
      a parameter is chosen, we compute an interval to vary that parameter over based
      off of the previous tuned value of that parameter and the interval type
      (arithmetic, geometric).  Not sure how this will work with discrete variables but
      we can at least use this for continuous parameters.
    - Add option to use meta_train() instead of train().
- Nice to have:
    - Plots to show progression of various hyperparameters during hyperparameter search
    - Include name of search parameter in name of results file for IC grid search

Tests:
- Refactoring:
    - Modify tests so that we don't have to save/load from/to disk during tests, which
      is possible now that we return checkpoints from train().
    - In tests/networks/test_trunk.py, test_backward() asserts that the generated
      observation batch contains data from all tasks, but the obs batch isn't explicitly
      constructed this way. It happens to pass by luck but we should enforce this so it
      doesn't randomly break if seed or something else changes.
- Additions:
    - Tests for solving simple openai gym environments in a small amount of time
- Nice to have:
