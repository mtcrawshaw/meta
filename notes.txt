Road map:
--> PPO for single task
    Multitask PPO for multi task learning
    Shared architecture for multi task learning
    PCGrad, sparse sharing, soft-layer ordering, and other baselines for multi task learning
    Learned loss convexity regularization for multi task learning
    MAML and other baselines for meta learning single task distribution
    Shared architecture for meta learning
    Hyperparameter optimization
    Tests of statistical significance

Current sprint:
    Convert List[RolloutStorage] back to RolloutStorage
    Set new continuous environment benchmark
    Move to settings file
    Move run_discrete and run_continuous into tests
    Make policy return distribution object, not arguments
    Add multiprocessing
--> Add GPU support
    Add tests for MetaWorld environments
    Add support for recurrent policies
    Test for Box((n1, n2))
    Fix comments, typing, and naming (episodes vs. rollouts)

Before merging to master:
- Use black, pylint, and mypy to clean up
- Make sure "pytest meta/" passes

Future additions:
- PPO training settings to add/consider:
    - Network architecture hyperparameters
    - Choice of optimizer and optimizer hyperparameters
    - Parameter sharing between actor and critic
    - Deterministic acting (i.e. always choose action with highest prob)
- Optimize data transfer to GPU

For tomorrow:
- Tests don't pass for GPU version because CPU and GPU forward pass differ slightly. A
  different action is chosen right off the bat, despite the same initial parameters and
  observation. Try comparing against ikostrikov GPU version, the saved baselines right
  now are just copies of the CPU baselines.
- Combine all discrete settings files into one, all continuous settings files into one
- Make tests shorter
- Suppress warnings

For presentation:
- Save out results file with EMA rewards
- Make plots from results file
- Compare against single-task networks?
- Add group indices to compare against one-hot task indices?
