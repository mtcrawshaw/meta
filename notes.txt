Road map:
--> PPO for single task
    Multitask PPO for multi task learning
    Shared architecture for multi task learning
    PCGrad, sparse sharing, soft-layer ordering, and other baselines for multi task learning
    Learned loss convexity regularization for multi task learning
    MAML and other baselines for meta learning single task distribution
    Shared architecture for meta learning
    Hyperparameter optimization
    Tests of statistical significance

Current sprint:
- Convert List[RolloutStorage] back to RolloutStorage
- Fix comments, typing, and naming (episodes vs. rollouts)
- Make policy return distribution object, not arguments
- Remove AddBias hack
- Move to settings file
- Move run_discrete and run_continuous into tests
- Add GPU support
- Add multiprocessing
- Add support for recurrent policies

PPO training settings to add/consider:
- Network architecture hyperparameters
- Choice of optimizer and optimizer hyperparameters
- Parameter sharing between actor and critic
- Deterministic acting (i.e. always choose action with highest prob)

