Road map:
    PPO for single task
--> Add MetaWorld support
    Splitting networks for MTL
    PCGrad, sparse sharing, soft-layer ordering, other baselines for MTL
    Splitting networks for meta learning
    MAML and other baselines for meta learning
    Hyperparameter optimization
    Tests of statistical significance
    Learned loss convexity regularization for MTL

Current sprint:
    Add support for single (fixed) MetaWorld environments
    Add support for multi-task MetaWorld benchmarks
--> Reconcile one-hot observations being normalized
    Add tests for MetaWorld environments
    Fix comments, typing, and naming (episodes vs. rollouts), suppress warnings, clean up train.py

Before merging to master:
- Use black, pylint, and mypy to clean up
- Make sure "pytest meta/" passes

Future additions:
- Test PPO for environments with action space Box((n1, n2))
- PPO training settings to add/consider:
    - Network architecture hyperparameters (more customized architectures)
    - Choice of optimizer and optimizer hyperparameters
    - Parameter sharing between actor and critic
    - Deterministic acting (i.e. always choose action with highest prob)
- Make tests deterministic
- Move tests outside of package
- Add feature so that task-index part of observation is not normalized

Possible additions:
- Optimize data transfer to GPU
- Different ways to collect data from multiple tasks. Cycle through tasks, randomly pick
  one, randomly pick one that is different from previous.
- Refactor RolloutStorage data generators. Put tensors into a dict. Can we just use good
  indexing to construct the batches instead of looping and stacking?

----------------------------------------------------------------------------------------
