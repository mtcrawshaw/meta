Road map:
    PPO for single task
    Add MetaWorld support
    Better evaluation
--> MT10/MT50 baselines
    Splitting networks for MTL
    PCGrad, sparse sharing, soft-layer ordering, other baselines for MTL?
    ML10/ML45 baselines
    Splitting networks for meta learning
    MAML, other baselines for meta learning?
    Hyperparameter optimization
    Tests of statistical significance
    Learned loss convexity regularization for MTL

Current sprint (MT10/MT50 baselines):
--> Shared trunk architecture
    ?

Current task (Shared trunk architecture):
    Add another directory level to package
--> Add new file for shared trunk architecture
    Naive forward pass
    Batch inputs of same task in forward pass

Before merging to master:
- Use black, pylint, and mypy to clean up
- Make sure "pytest meta/" passes

----------------------------------------------------------------------------------------

Future additions:
- Test PPO for environments with action space Box((n1, n2))
- PPO training settings to add/consider:
    - Network architecture hyperparameters (more customized architectures)
    - Choice of optimizer and optimizer hyperparameters
    - Parameter sharing between actor and critic
    - More learning rate schedule options
        - torch.optim.lr_scheduler.ReduceLROnPlateau
        - cyclic LR
        - ?
    - Various network initialization schemes
    - Value function normalization
    - Independent options for normalizing observations and rewards

Possible additions:
- Different ways to collect data from multiple tasks. Cycle through tasks, randomly pick
  one, randomly pick one that is different from previous.
- Compute standard deviation for success rate so that mean + stdev < 1 i.e. makes sense
  for a variable in the range [0, 1]. Easy way to do this is to assume that tan^(-1)(X)
  is normally distributed instead of X, compute mean + stdev and mean - stdev, then take
  tan of everything tp put it back into interval [0, 1].
- Rendering of episode videos
- Time various parts of the training process, save along with metrics
- Plots to show progression of various hyperparameters during hyperparameter search
- More robust way to save repository version along with results
- Determine whether or not policy has solved environment (N successes in a row, usually)
- Catch KeyboardInterrupt during training to optionally stop training early

Refactoring:
- Move tests outside of package
- Move optimizer and learning schedule outside of ppo.py in order to generalize training
  to CV and NLP. This is way in the future. It will be a pain because we will have to
  call optimizer.step() at each PPO epoch. Maybe we can initialize it in train.py and
  just pass a reference in to the policy.
- Optimize data transfer to GPU
- Refactor RolloutStorage data generators. Put tensors into a dict. Can we just use good
  indexing to construct the batches instead of looping and stacking?
- Refactor naming in PPO implementation to clarify between episodes, rollouts, and
  trajectories
- Use Metrics objects outside of train() instead of state dictionary.
- Rename PPO parameters (i.e. gamma -> discount)
- Get rid of redundant functionality for saving metrics
- Move metric comparison outside of train() and hyperparameter_search(). We wouldn't
  have to do so much bullshit to save/load/compare metrics if the functionality was
  modular
- Get rid of redundant settings inside and outside base_train_config for hyperparameter
  search, i.e. seed, save name, etc
- Clean up hyperparameter_search() a lot:
  - Break it into separate files
  - Get rid of repeated code in different search strategies
  - Clean up redundant options between "base_train_config" and "hp_config"
  - A billion other things, it's disgusting
- Change main.py to accept a command name ("train" or "tune") and a config

----------------------------------------------------------------------------------------

