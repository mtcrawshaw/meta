Road map:
--> PPO for single task
    Add MetaWorld support
    Splitting networks for MTL
    PCGrad, sparse sharing, soft-layer ordering, other baselines for MTL
    Splitting networks for meta learning
    MAML and other baselines for meta learning
    Hyperparameter optimization
    Tests of statistical significance
    Learned loss convexity regularization for MTL

Current sprint:
    Convert List[RolloutStorage] back to RolloutStorage
    Set new continuous environment benchmark
    Move to settings file
    Move run_discrete and run_continuous into tests
    Make policy return distribution object, not arguments
    Add multiprocessing
--> Add GPU support
    Add support for recurrent policies
    Test for Box((n1, n2))
    Fix comments, typing, and naming (episodes vs. rollouts)

Before merging to master:
- Use black, pylint, and mypy to clean up
- Make sure "pytest meta/" passes

Future additions:
- PPO training settings to add/consider:
    - Network architecture hyperparameters
    - Choice of optimizer and optimizer hyperparameters
    - Parameter sharing between actor and critic
    - Deterministic acting (i.e. always choose action with highest prob)
- Optimize data transfer to GPU

----------------------------------------------------------------------------------------

For right now:
- Get GPU tests to pass
- Make tests shorter
- Suppress warnings

Debugging:
So I've put in a shit ton of print statements through the training pipeline in this
branch and also in the original_benchmark branch, to try to spot the difference. It
seems that the call to nn.utils.clip_grad_norm_ returns different results for the two
versions. I am guessing that this is happening because the parameterization between the
two models is slightly different, so when the gradient gets rescaled by some
multiplicative factor in the two versions, each new parameter is slightly off.
Investigate this next time.
