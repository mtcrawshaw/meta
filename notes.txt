

Road Map


The goal of this branch is to implement Complete Knowledge Distillation within our
current training infrastructure. So far I've added CKDTrainer, but I think I was getting
ahead of myself and I backtracked to just playing with CKD in a standalone script called
scripts/ckd_sandbox.py. So far, the script is able to produce a SymPy expression that
approximates the output of a given MLP (with exponential activation function), and we
can evaluate this SymPy expression by plugging in Tensors for the symbols representing
input values + network parameter values. It won't be too hard to extend this and
actually implement CKD, we just have to include the ability to produce the SymPy
expression corresponding to two networks, take the squared difference of the
expressions, then evaluate it with Tensor operations. This is our loss, and we should be
able to backpropagate through it. To train a student network to mimic a teacher. The
script shows that we can decently approximate the output of a tiny network with this
polynomial represented by the SymPy expression, but the limiting factor here is compute
time. For a network with 3 layers, with input/hidden/output size <= 4, and expanding the
power series of the exponential out for 5 terms, it didn't finish computing the SymPy
expression within 15 minutes. I don't know how long it would've taken, I just stopped
it. And that is a TINY network. This may be bad enough to kill the idea entirely.
Possible ways around this: move from SymPy to a more optimized computer algebra system
written in a different language. This is gonna be super messy, not just because we have
to interface to that code through Python, but because we NEED a representation of the
expression accessible from Python so that we can perform torch Tensor operations
according to that expression. Alternatively, we could write a very simple symbolic
computation system that would only handle the operations necessary to compute the
network output, and we could do this in C. Still, this might not be enough to save it.
Given that the computation time grew that fast for such a tiny network, we will likely
need many orders of magnitude speedup to make this possible. Parallelization will help
this, but only by a factor equal to the number of threads on the machine, which is 12
for me. The most extreme option is to employ massive parallelization by writing CUDA
code to perform the symbolic computation on the GPU. This is technically a highly
parallelizable operation, since we are performing matrix multiplication, just
symbolically. My only worry about this is the fact that the computation isn't
homogeneous across cores, as it would be if we were performing numerical matrix
multiplication. I think that this homogeneity is important for maximizing speedup from
parallelization on the GPU, so this might kill the GPU idea. For now, just try to extend
the current script to handle the student/teacher setup and perform training. If this
works okay, we should think seriously about increasing efficiency and possibly bring it
to Jana.

Tasks:
    Approximate output of a single network with a symbolic polynomial
    Approximate difference between two networks (on sampled data)
    Approximate difference between two networks over entire data distribution
    Train student network to match teacher with supervised learning
    Train student network to match teacher with complete knowledge distillation
    Convert supervised learning to use a fixed dataset with train/test evaluation
    Extend to ReLU activation
--> Convert loss expression computation/evaluation to work with sparse matrices
    Add option to measure accuracy of polynomial approximation to each network
    Move to CKDTrainer?

Other directions worth exploring:
- Extend to common activation functions (ReLU, sigmoid) with polynomial approximations.
  The approximations can be computed by minimizing the L2 distance between the desired
  activation function and a polynomial of given degree either over a fixed neighborhood
  of the origin or over a normally distributed real random variable.
- Support for more complex architectures:
  - Convolutional layers
  - Batch normalization layers
  - Residual blocks
  - ?
- Implement CKD for convolutional networks
- Compute the number of terms in the final expression as a function of the number of
  layers, the hidden size, input size, output size, and the power series truncation
  constant. Do the same thing for convolutional networks, which will likely have a much
  shorter error expression.
- Replace large sums with SymbolicSum during computation of loss function. Some of the
  sums that we have to work with during the loss function computation (such as those
  resulting from applying an activation function's power series to an input) are such
  that the i-th term of the sum is easily expressed as a closed form function of i. This
  means, instead of representing a sum as a list over all of the summands, it can simply
  be represented with the function mapping i to the i-th term. The main advantage of
  this is memory and time savings. The memory savings are clear. Time can be saved by
  replacing a procedure that performs the same operation on each of the summands with
  another procedure that only performs that operation a single time. This also has the
  advantage that this doesn't require us to prematurely truncate any power series, so we
  can compute and cache the final polynomial, then quickly try a varying number of power
  series truncation constants. The only downside is I'm not sure how significant the
  memory/time savings will be, since the sums with the largest number of terms will be
  result from the matrix-vector operations, and these sums cannot be made symbolic.
  Still, it's an option.

Notes for current task:
The current version contains a working implementation of CKD that uses matrix operations
to evaluate the CKD loss, but still produces the matrices involved in this operation
using SymPy. In addition, the matrix operations are performed with the dense equivalent
of a sparse matrix because torch doesn't currently support exponentiation with sparse
tensors. For now, we can go ahead and eliminate the dependence on SymPy by writing
functionality to compute the power and coefficient matrices themselves with matrix
operations. Once this is done, we're kind of stuck: we need torch functionality to
compute exponentials with sparse tensors. If/when this happens, we just need to make a
small change to the loss evaluation (described in a comment in `evaluate_error()` in
order to keep memory costs managable, then it should be good to try on a larger network.


----------------------------------------------------------------------------------------


Issues


- Save/load tests for tuning are failing. For some reason the results are inconsistent
  between running a single tune run all the way through and saving/loading that same run
  in the middle. This is likely due to an ability to mimic random decisions after
  loading, but this wasn't the case before the switch to MetaWorld v2 environments.
- MetaWorld observation tests for MLX are failing. Assumptions about variation of goals,
  initial hand positions, and initial object positions aren't met by observations
  returned from rollouts. This is due to issues in the MetaWorld repo, but we should
  ensure that our tests pass once their issues are resolved.
- MetaWorld MLX environments yield different goals for different processes. This is due
  to the fact that the goals are randomly generated at run-time, and each process is
  given a different random seed. One way to fix this is to give each process the same np
  seed (which can do by setting `same_np_seed` to True in the config file), but this may
  cause other unwanted consequences. Resolve this before running experiments on MLX.


----------------------------------------------------------------------------------------


Practices


Before merging to master:
- Use black, pylint, and mypy to clean up
- Make sure "pytest tests/" passes


----------------------------------------------------------------------------------------


Future Plans


DevOps:
- Make sure that a clean install works properly (repo contains all data necessary for
  tests, requirements contains all requirements, etc)
- Move test files (configs, results, metrics, etc.) into a separate subdirectory
- Containerize this shit

Training infrastructure:
- Refactoring:
    - Optimize data transfer to GPU
    - Get rid of redundant functionality for saving metrics
    - Move metric comparison outside of train() and hyperparameter_search(). We wouldn't
      have to do so much bullshit to save/load/compare metrics if the functionality was
      modular
    - Fix learning rate schedule behavior when resuming runs. Right now, if a run
      finishes, and then it is resumed and trained for a longer period of time, the
      learning rate will not align with the original schedule. Really this could be
      fixed by creating two different loading modes: resume and extend. Resume will
      follow the original schedule for parameters, and extend will ditch the old one and
      follow the newly given one.
    - Fix cosine learning rate schedule definition (T_max should be num_updates - 1) and
      create new corresponding baselines
    - Create config/architecture_config object so we don't have to mess with
      string-keyed dictionaries everywhere
    - Change metrics measuring to be more like logging. Then we can just add anything to
      metrics from anywhere. The metrics object should just accept a Dict[str,
      List[float]], where keys are metric names and values are new metric values to add
      to that metric's history. If the string is new, then we just start a history for a
      new metric. This will be a lot more flexible than hand-coding in each individual
      metric.
    - Convert string literals in config (network architecture type, lr schedule type,
      environment names, etc) into enums.
    - Fix reproducibility issue with meta RL. At the moment, the goals sampled in the
      Meta-World environments are inconsistent between training runs, but only during
      meta-testing. Right now, we are only testing that the baselines line up for
      meta-training, which I actually think is fine for now since meta-testing is
      essentially just an evaluation, and is much shorter in practice than
      meta-training. The goals are sampled from using np.ndarray.sample(), and we do set
      the np seeds at the beginning of both meta-training and meta-testing. The fact
      that this is only happening during meta-training makes me feel like this is an
      issue with setting the seed twice, but I don't see how that could be true. It may
      also be specific to the Meta-World environments somehow.
    - Remove in-line imports for Meta-World and replace with try-catch imports at the
      top of each file.
- Additions:
    - Training options:
        - Choice of optimizer and optimizer hyperparameters
        - Learning rate schedules (ReduceLROnPlateau, cyclic LR)
    - Multi-task architecture support for environments with observations whose
      dimension is greater than 1
    - Add tests checking saved/loaded values for saving/loading training runs. This
      can't be done right now since the training function is so monolithic, we can't
      "look inside" and check the values of the local variabes of that function. Once
      this functionality is more modular we should add these tests. For now I just
      manually inspected the values after loading and created a baseline to compare
      metrics against. The same goes for saving/loading hyperparameter random search
      runs (the other types of hp search are reproducible when saving/loading, so
      there's no need for it there).
    - Allow for training on a sequence of configs by specifying a list of values in the
      config file instead of a single value. Just getting training to work like this
      will be easy, the more annoying part will be saving and loading over these
      compound runs.
    - Measure training metrics on each individual task during multi-task training,
      create a single plot with a subplot for each individual task.
    - Saving/loading for meta-learning runs.
    - Add option to meta-test by loading a meta-trained model.
- Nice to have:
    - Compute standard deviation for success rate so that mean + stdev < 1 i.e. makes
      sense for a variable in the range [0, 1]. Easy way to do this is to assume that
      tan^(-1)(X) is normally distributed instead of X, compute mean + stdev and mean -
      stdev, then take tan of everything to put it back into interval [0, 1].
    - Time various parts of the training process, save along with metrics.
    - More robust way to save repository version along with results.
    - Catch KeyboardInterrupt during training to optionally stop training early while
      still saving out final metrics and plot.
    - When training starts and results are not being saved, notify user and give the
      option to turn saving on.
    - Make it so that when resuming training from a checkpoint, training happens exactly
      as it would had if training had never been stopped. Not sure if this is possible,
      the main obstacle being pickling/loading the environment state, and mimicking
      random decisions.
    - Monitor system resource utilization and recommend changes in config for maximum
      utilization, i.e. bigger batch sizes to increase GPU utilization, less processes
      if thrashing is present.
    - Output hyperparameter tables in performance plots.
    - Tests of statistical significance of results when comparing different methods.
    - History of saved experiments: Right now experiment history is only preserved
      through files written to "results/". It would be nice to access a list of
      experiments sorted by date and including name and some metrics.

RL training:
- Refactoring:
    - Refactor RolloutStorage data generators. Put tensors into a dict. Can we just use
      good indexing to construct the batches instead of looping and stacking?
    - Refactor naming in PPO implementation to clarify between episodes, rollouts, and
      trajectories
    - Rename PPO parameters (i.e. gamma -> discount)
    - Figure out the difference between our setup and other setups (such as multi-task
      PPO in the Meta-World paper) which use a linear feature baseline.
- Additions:
    - Test PPO for environments with action space Box((n1, n2))
    - Independent options for normalizing observations and rewards
    - Network architecture hyperparameters:
        - Parameter sharing between actor and critic (and separate recurrent layers)
        - Recurrent layers throughout network, different types of recurrent layers
        - Value function normalization
    - Options to collect data from multiple tasks. Cycle through tasks, randomly pick
      next, randomly pick next that is different from previous. One option should ensure
      that multi-process training is collecting data from tasks as uniformly as
      possible.
- Nice to have:
    - Rendering of episode videos
    - Determine whether or not policy has solved environment (N successes in a row,
      usually)
    - Deparallelization factor as an option. We include an option called
      `rollouts_per_process` that dictates how many rollouts each process should execute
      during a training step. Ideally, running 4 processes with 1 rollout per process
      would be the same as running 2 processes with 2 rollouts per process, but having
      the option to choose allows us to maximize the efficiency of whatever hardware we
      are running on. On my laptop, the limiting factor is RAM, so decreasing
      parallelization (i.e. increasing rollouts_per_process) is the way to go to avoid
      thrashing. The reason I haven't implemented this yet is because there isn't really
      a good answer to the following: should we reset the environment between
      consecutive rollouts for the same process? If the answer is yes, then we will
      never get past the first `rollout_length` steps of an episode, but if the answer
      is no, then running multiple rollouts per process is effectively the same as just
      increasing the rollout length. What we really want is to be able to reset the
      environment between rollouts from the same process, then restore the environment
      state of each rollout after the training step. However, this requires storing the
      environment state (don't wanna do that) and would greatly increase memory cost,
      which is the whole reason we want a deparallelization option in the first place.
      The increase in memory wouldn't actually be that bad, because even though the
      amount of memory needed for each process would increase by a factor of
      `rollouts_per_process`, that memory would be partitioned into
      `rollouts_per_process` pieces which each only need to exist in RAM one at a time,
      so I don't think we would see thrashing here. Still, storing and restoring the
      environment state is a pain and for now I'm just gonna increase the rollout
      length.

SL training:
- Refactoring:
  - Revise evaluation so that we are evaluating on more than just a single batch.
  - Find nicer way to handle dataset input sizes than the hard-coded dictionary we have
    now. Is it possible to get the size of inputs/outputs without having to store them
    explicitly?
- Additions:
  - MLP support for supervised learning.
  - In multi-task learning, option to enforce constant sum of loss weights (+ positive
    loss weights)
- Nice to have:

Network architectures:
- Refactoring:
    - Remove redundant calls to "self.to(device)" in nested modules?
- Additions:
    - Architecture hyperparameters:
        - Varying network size in hidden layers
        - Network initialization options
    - Convolutional network features:
        - Max pooling
        - Batch normalization
    - An option to turn on checks during inference (safe mode). We would check that each
      observation in a multi-task setting ends with a one-hot tensor of size
      `self.num_tasks`, and that task gradients from get_task_grads() from
      MultiTaskSplittingNetwork have the same zero padding structure before finding
      their difference, etc. This way we still have the option to check and be safe, but
      we don't have to waste computation time doing so if we don't want to. This also
      applies to checks of tensor structure in forward() of MultiTaskTrunkNetwork,
      get_loss() from PPOPolicy, and probably a bunch of other places.
- Nice to have:

Splitting networks:
- Refactoring:
    - Refactor MultiTaskSplittingNetwork.get_task_grad_diffs() to only compute pairwise
      differences between task gradients for tasks that share a copy. Right now it is
      computed for each pair of tasks at each region. We could also parallelize the
      computation across regions but I'm not quite sure how to do that.
    - In splitting networks, modify task gradient computation so that we don't perform a
      redundant backwards pass. Right now, we pass over each task loss, then zero
      everything out and pass over the sum of losses. We don't actually need to do that
      last backward pass, but we need to do some stuff to get the task losses without
      zeroing out gradients between each backward pass. Really we just need to take the
      successive differences of gradients, and we also need to change the format of the
      backward pass in general so that .backward() does get called on the entire loss if
      we aren't using splitting networks. Also, the problem is twice as bad as we
      originally thought: we are calling backwards on each task loss twice! Once in
      check_for_splits() for the actor and again for the critic. Fixing this
      inefficiency is gonna be messy.
    - Refactor MultiTaskSplittingNetwork.update_grad_stats() to get task flags in a way
      that is less brittle than checking whether the gradient vector is all zero. Maybe
      this is an edge case that we actually want to account for. We could probably get
      this info from `task_losses`.
- Additions:
    - Option to reset splitting statistics after a split is performed (this means we
      have to split regions with highest z-scores first)
    - Try training splitting networks by measuring gradient variance online, just
      with a smaller EMA alpha value. It might be that an EMA which moves too slowly
      will yield a variance which is larger than it should be, making the test for
      splitting not sensitive enough.
    - Support the inclusion of task index in input to splitting/meta-splitting networks.
      This is pretty messy, since we need to distinguish tasks from meta-training
      distribution and tasks from meta-testing distribution, but we also need to use the
      same layers between meta-training nad meta-testing. We can't just include the task
      index, because there may not be the same number of training/testing tasks. Even if
      there are, the network would think that an observation from testing task 1
      actually came from training task 1. If we try to use one-hot vectors whose size is
      the total number of training+testing tasks, then we end up in a situation where
      the weights used to transform the testing task inputs never get trained during
      meta-training, and then get frozen for meta-testing. The only solution I can think
      to this is to unfreeze specificially those weights during meta-testing, but this
      is not super clean and also goes against the spirit of modularity in splitting
      networks? Not sure.
- Nice to have:
    - In actor/critic network, add splitting for action_logstd. Right now each task just
      has a separate set of these parameters.
    - Allow weights/biases to be split independently

Hyperparameter search:
- Refactoring:
    - Get rid of redundant settings inside and outside base_train_config for
      hyperparameter search, i.e. seed, save name, etc
    - Get rid of repeated code in different search strategies
    - Clean up redundant options between "base_train_config" and "hp_config"
    - Change hyperparameter search to allow non-unique leaf node names in config. After
      allowing separate architecture configs for actor and critic networks, we have a
      bunch of parameters with non-unique names. What this means is that parameters with
      the same name will be tied together during the search process. To fix this, we
      need to change the way that we specify search spaces for parameters. Right now
      each search space is tied to a parameter just by its name, which is why we needed
      unique names, but what we really need is for each search space to be tied to the
      path through the config tree to the corresponding leaf. To do this, we should just
      specify the search spaces within the base train config dictionary. Instead of
      having individual values within this config, we just have a list of values, or
      another dict which describes the search space.
- Additions:
    - An actual good random search: Check Bengio et al. (2014ish) for a competitive
      random hyperparameter search technique
    - Another hp search strategy: IC grid search, but which parameter to vary is chosen
      automatically. Each parameter is varied once to get an initial tuning for each
      one, then the next parameter to tune is sampled from a distribution in which the
      probability that a parameter gets chosen is equal to some normalized estimate of
      the potential gain from tuning that parameter. A natural choice is to use the
      standard deviation of the rewards from the previous tuning as this estimate. Once
      a parameter is chosen, we compute an interval to vary that parameter over based
      off of the previous tuned value of that parameter and the interval type
      (arithmetic, geometric).  Not sure how this will work with discrete variables but
      we can at least use this for continuous parameters.
    - Add option to use meta_train() instead of train().
- Nice to have:
    - Plots to show progression of various hyperparameters during hyperparameter search
    - Include name of search parameter in name of results file for IC grid search
    - Make it so that tune runs are consistent with train runs. This means if we run the
      same config through "train" as with one step of "tune", they will create the same
      results. This will likely involve separating the random seeds used in each.

Tests:
- Refactoring:
    - Modify tests so that we don't have to save/load from/to disk during tests, which
      is possible now that we return checkpoints from train().
    - In tests/networks/test_trunk.py, test_backward() asserts that the generated
      observation batch contains data from all tasks, but the obs batch isn't explicitly
      constructed this way. It happens to pass by luck but we should enforce this so it
      doesn't randomly break if seed or something else changes.
    - Make tests faster by decreasing training lengths and replacing complex
      environments (particularly MetaWorld) with simpler ones.
- Additions:
    - Tests for solving simple openai gym environments in a small amount of time
- Nice to have:

Style:
- Refactoring:
  - Change keywords for string options to name of corresponding classes. Example,
    architecture types can just be MLPNetwork, ConvNetwork, etc.
  - Remove as many default arguments as possible.
  - Clean up imports so that we import from a subpackage's __init__.py instead of from
    each file in that subpackage.
  - Convert config into an object or just store config values as members in the relevant
    class, so that we can get rid of magic strings everywhere.
  - Fix security issues with `eval()`. Anytime `eval()` is used, we need to check that
    the input is a member of a fixed whitelist.
- Additions:
- Nice to have:
