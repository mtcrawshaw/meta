Road map:
    PPO for single task
    Add MetaWorld support
--> Better evaluation
    MT10/MT50 baselines
    Splitting networks for MTL
    PCGrad, sparse sharing, soft-layer ordering, other baselines for MTL?
    ML10/ML45 baselines
    Splitting networks for meta learning
    MAML and other baselines for meta learning
    Hyperparameter optimization
    Tests of statistical significance
    Learned loss convexity regularization for MTL

Current sprint:
    EMA of rewards instead of looking at past N
    Option to plot mean + std dev
    Evaluate success rate during training
    Plot success rate
    Saving settings, metrics under results
    Move legend outside of axes
    Add success rate computation for gym environments
    Get tests to pass
    Metrics tests
    Evaluate metrics after training
    Add final metrics table to plot
    Deterministic acting
    Learning rate schedules
    Evaluation during training
--> Hyperparameter search
    Additional training tricks/options

Current task (Hyperparameter search):
--> Add grid search
    Add iterated constrained grid search

Next task (Additional training tricks/options):
    Normalize observation without transition
    Linear learning rate schedule
    Initialization (https://arxiv.org/abs/2006.05990)

Before merging to master:
- Use black, pylint, and mypy to clean up
- Make sure "pytest meta/" passes

----------------------------------------------------------------------------------------

Future additions:
- Test PPO for environments with action space Box((n1, n2))
- PPO training settings to add/consider:
    - Network architecture hyperparameters (more customized architectures)
    - Choice of optimizer and optimizer hyperparameters
    - Parameter sharing between actor and critic
    - More learning rate schedule options
        - linear schedule
        - torch.optim.lr_scheduler.ReduceLROnPlateau
        - cyclic LR
        - ?
    - Add feature so that task-index part of observation is not normalized for MTL
    - Various network initialization schemes
    - Value function normalization
    - Independent options for normalizing observations and rewards
- More explicit way to handle configs. Would be nice to have a formal specification of
  the config values, as well as a programmatic difference between different types of
  configs

Possible additions:
- Different ways to collect data from multiple tasks. Cycle through tasks, randomly pick
  one, randomly pick one that is different from previous.
- Compute standard deviation for success rate so that mean + stdev < 1 i.e. makes sense
  for a variable in the range [0, 1]. Easy way to do this is to assume that tanh^(-1)(X)
  is normally distributed instead of X, compute mean + stdev and mean - stdev, then take
  tanh of everything.
- Rendering of episode videos
- Time various parts of the training process, save along with metrics
- Plots to show progression of various hyperparameters during hyperparameter search
- More robust way to save repository version along with results
- Determine whether or not policy has solved environment (N successes in a row, usually)

Refactoring:
- Move tests outside of package
- Move optimizer and learning schedule outside of ppo.py in order to generalize training
  to CV and NLP. This is way in the future. It will be a pain because we will have to
  call optimizer.step() at each PPO epoch. Maybe we can initialize it in train.py and
  just pass a reference in to the policy.
- Fix EOFError when multiprocessing
- Optimize data transfer to GPU
- Refactor RolloutStorage data generators. Put tensors into a dict. Can we just use good
  indexing to construct the batches instead of looping and stacking?
- Refactor naming in PPO implementation to clarify between episodes, rollouts, and
  trajectories
- Use Metrics objects outside of train() instead of state dictionary.
- Rename PPO parameters (i.e. gamma -> discount)

----------------------------------------------------------------------------------------

Environments to run PPO experiments on:
- Ant
- Hopper
- HalfCheetah
- MetaWorld single task environments
- MT10
- MT50
- Move optimizer and learning schedule outside of ppo.py in order to generalize training
  to CV and NLP. This is way in the future. It will be a pain because we will have to
  call optimizer.step() at each PPO epoch. Maybe we can initialize it in train.py and
  just pass a reference in to the policy.
- Fix EOFError when multiprocessing

Training settings to try:
- More parallelization + longer rollouts (bigger batches, GPU utilization is small)
- Go over experience multiple times (num_ppo_epochs > num_minibatch)
