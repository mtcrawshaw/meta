Road map:
    PPO for single task
    Multi-task/MetaWorld support
    Training tools
--> MT10/MT50 methods (baselines + splitting networks)
    ML10/ML45 methods (baselines + meta splitting networks)
    Hyperparameter optimization
    Tests of statistical significance
    Learned loss convexity regularization for MTL

Current sprint (MT10/MT50 baselines):
--> Shared trunk architecture
    Splitting networks
    PCGrad
    Other baselines (
        Branched MT networks,
        soft layer ordering,
        modular meta learning (code available),
        AdaShare,
        Piggyback (code available),
        sparse sharing (code available),
        routing networks (code available),
    )
    Refactoring

Current task (Shared trunk architecture):
    Add another directory level to package
    Create base network class, extend it with PolicyNetwork
    Shared trunk network with naive forward pass
    Add trunk tests
    Establish shared trunk baselines
    Batch inputs from the same task in forward pass
    Add option to include/exclude task index to forward pass
    Evaluate trunk performance on MT10
--> Refactoring

Notes for current task:
Implementing saving/loading of tuning runs. Just finished implementing it for grid
search, as well as a unit test case, though we need to add a check that the loaded
config is equal to the current config. Need to do the same for IC grid search and random
search, though random search will need to measure against a saved baseline since we
won't be able to 100% accurately reproduce training as if it had never been paused in
the case of resuming of a random search run. Consolidate save/load configs with default
configs, and change config settings for tests around so that success is only used as a
metric for CartPole, and reward is used for LunarLander. Make sure that
tune_results_equal() works for results from random/IC grid search runs.

----------------------------------------------------------------------------------------

Before merging to master:
- Use black, pylint, and mypy to clean up
- Make sure "pytest tests/" passes

----------------------------------------------------------------------------------------

Future additions:
- Test PPO for environments with action space Box((n1, n2))
- PPO training settings to add/consider:
    - Network architecture hyperparameters
      - Parameter sharing between actor and critic
      - Option to have separate recurrent layers
      - Recurrent layers throughout network, different types of recurrent layers
    - Choice of optimizer and optimizer hyperparameters
    - More learning rate schedule options
        - torch.optim.lr_scheduler.ReduceLROnPlateau
        - cyclic LR
        - ?
    - Various network initialization schemes
    - Value function normalization
    - Independent options for normalizing observations and rewards
- Multi-task architecture support for environments with observation spaces with
  dimension greater than 1
- When it comes time to add training for CV and NLP tasks, decouple the current network
  objects into model objects and network objects. For example, for the current RL setup
  we will have an actor critic model, which has two network objects as members, one
  actor network and one critic network. Right now the only available network objects
  have to be actor critic networks.
- Add tests checking saved/loaded values for saving/loading training runs. This can't be
  done right now since the training function is so monolithic, we can't "look inside"
  and check the values of the local variabes of that function. Once this functionality
  is more modular we should add these tests. For now I just manually inspected the
  values after loading and created a baseline to compare metrics against. The same goes
  for saving/loading hyperparameter random search runs (the other types of hp search
  should be reproducible).

Possible additions:
- Different ways to collect data from multiple tasks. Cycle through tasks, randomly pick
  one, randomly pick one that is different from previous.
- Compute standard deviation for success rate so that mean + stdev < 1 i.e. makes sense
  for a variable in the range [0, 1]. Easy way to do this is to assume that tan^(-1)(X)
  is normally distributed instead of X, compute mean + stdev and mean - stdev, then take
  tan of everything tp put it back into interval [0, 1].
- Rendering of episode videos
- Time various parts of the training process, save along with metrics
- Plots to show progression of various hyperparameters during hyperparameter search
- More robust way to save repository version along with results
- Determine whether or not policy has solved environment (N successes in a row, usually)
- Catch KeyboardInterrupt during training to optionally stop training early
- Regularization terms from AdaShare when learning architecture
- Make it so that, when resuming training from a checkpoint, training happens exactly as
  it would had if training had never been stopped. Not sure if this is possible, the
  main obstacle being pickling/loading the environment state, and mimicking random
  decisions.

Refactoring:
- Move optimizer and learning schedule outside of ppo.py in order to generalize training
  to CV and NLP. This is way in the future. It will be a pain because we will have to
  call optimizer.step() at each PPO epoch. Maybe we can initialize it in train.py and
  just pass a reference in to the policy.
- Optimize data transfer to GPU
- Refactor RolloutStorage data generators. Put tensors into a dict. Can we just use good
  indexing to construct the batches instead of looping and stacking?
- Refactor naming in PPO implementation to clarify between episodes, rollouts, and
  trajectories
- Use Metrics objects outside of train() instead of state dictionary.
- Rename PPO parameters (i.e. gamma -> discount)
- Get rid of redundant functionality for saving metrics
- Move metric comparison outside of train() and hyperparameter_search(). We wouldn't
  have to do so much bullshit to save/load/compare metrics if the functionality was
  modular
- Get rid of redundant settings inside and outside base_train_config for hyperparameter
  search, i.e. seed, save name, etc
- Clean up hyperparameter_search() a lot:
  - Get rid of repeated code in different search strategies
  - Clean up redundant options between "base_train_config" and "hp_config"
  - A billion other things, it's disgusting
- Determine whether or not we save time by skipping the check that the end of each
  observation is a one-hot vector in meta/networks/mt_trunk.py, forward()
- Change hyperparameter search to allow non-unique leaf node names in config
- Implement checkpointing/resuming training for hyperparameter search
- Fix learning rate schedule behavior when resuming runs. Right now, if a run finishes,
  and then it is resumed and trained for a longer period of time, the learning rate will
  not align with the original schedule. Really this could be fixed by creating two
  different loading modes: resume and extend. Resume will follow the original schedule
  for parameters, and extend will ditch the old one and follow the newly given one.
- Fix cosine learning rate schedule definition (T_max should be num_updates - 1) and
  create new corresponding baselines
- Get rid of warnings that occur during tests
- Make sure that a clean install works properly (repo contains all data necessary for
  tests, requirements contains all requirements, etc)
- Change hp tests so that success rate is used for CartPole, and reward is used for
  LunarLander. After doing this, consolidate interrupted/resumed configs with the
  default configs

