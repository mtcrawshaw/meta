

Continual Batch Normalization


Experiments investigating the drawbacks of batch normalization in the continual learning
setting.


Sprints:
    Naive continual learning on split MNIST
    Continual batch normalization protocols + experiments
--> Manifold optimization

Current sprint (Continual learning on split MNIST):
    SGDG and AdamG
--> PSI-SGD
    Experiments for PSI-SGD + A-GEM

Refactoring/additions for this branch:
- Support for learning rate schedules in ContinualTrainer
- Structured way to evaluate testing after training on all tasks. Right now it just
  prints out values, but we should add them to Metrics, plot them, etc.

Other refactoring:
- Generally, there should be less shared functionality between trainers. Right now,
  train.py runs a single training loop over which metrics are aggregated. This is
  specific to the case of training a single model, and isn't applicable to continual
  learning or meta learning. We sort of got around that with the meta_train function,
  but this is really just a temporary workaround. Each trainer should run its own
  training loop. This is a big refactor, though.
- MTRegression should use the same data even when the number of tasks varies. Right now
  we have a different copy of the data for MTRegression10, MTRegression20, etc.
- Experiment should store all results in a single directory in results/, and the
  individual results should be subdirectories of this main directory.

Current task:
- Bare PSI-SGD (DONE)
- PSI-SGD with momentum
- PSI-SGD for convolutional networks
- Add in upper bound for neuron norms for overflow protection?

----------------------------------------------------------------------------------------


Issues


- Save/load tests for tuning are failing. For some reason the results are inconsistent
  between running a single tune run all the way through and saving/loading that same run
  in the middle. This is likely due to an ability to mimic random decisions after
  loading, but this wasn't the case before the switch to MetaWorld v2 environments.
- MetaWorld observation tests for MLX are failing. Assumptions about variation of goals,
  initial hand positions, and initial object positions aren't met by observations
  returned from rollouts. This is due to issues in the MetaWorld repo, but we should
  ensure that our tests pass once their issues are resolved.
- MetaWorld MLX environments yield different goals for different processes. This is due
  to the fact that the goals are randomly generated at run-time, and each process is
  given a different random seed. One way to fix this is to give each process the same np
  seed (which can do by setting `same_np_seed` to True in the config file), but this may
  cause other unwanted consequences. Resolve this before running experiments on MLX.
- A bunch of SL training tests are currently failing, because the baselines were never
  updated.


----------------------------------------------------------------------------------------


Practices


Before merging to master:
- Use black, pylint, and mypy to clean up
- Make sure "pytest tests/" passes


----------------------------------------------------------------------------------------


Future Plans


DevOps:
- Make sure that a clean install works properly (repo contains all data necessary for
  tests, requirements contains all requirements, etc)
- Move test files (configs, results, metrics, etc.) into a separate subdirectory
- Containerize this shit

Training infrastructure:
- Refactoring:
    - Optimize data transfer to GPU
    - Get rid of redundant functionality for saving metrics
    - Move metric comparison outside of train() and hyperparameter_search(). We wouldn't
      have to do so much bullshit to save/load/compare metrics if the functionality was
      modular
    - Fix learning rate schedule behavior when resuming runs. Right now, if a run
      finishes, and then it is resumed and trained for a longer period of time, the
      learning rate will not align with the original schedule. Really this could be
      fixed by creating two different loading modes: resume and extend. Resume will
      follow the original schedule for parameters, and extend will ditch the old one and
      follow the newly given one.
    - Fix cosine learning rate schedule definition (T_max should be num_updates - 1) and
      create new corresponding baselines
    - Create config/architecture_config object so we don't have to mess with
      string-keyed dictionaries everywhere
    - Change metrics measuring to be more like logging. Then we can just add anything to
      metrics from anywhere. The metrics object should just accept a Dict[str,
      List[float]], where keys are metric names and values are new metric values to add
      to that metric's history. If the string is new, then we just start a history for a
      new metric. This will be a lot more flexible than hand-coding in each individual
      metric.
    - Convert string literals in config (network architecture type, lr schedule type,
      environment names, etc) into enums.
    - Fix reproducibility issue with meta RL. At the moment, the goals sampled in the
      Meta-World environments are inconsistent between training runs, but only during
      meta-testing. Right now, we are only testing that the baselines line up for
      meta-training, which I actually think is fine for now since meta-testing is
      essentially just an evaluation, and is much shorter in practice than
      meta-training. The goals are sampled from using np.ndarray.sample(), and we do set
      the np seeds at the beginning of both meta-training and meta-testing. The fact
      that this is only happening during meta-training makes me feel like this is an
      issue with setting the seed twice, but I don't see how that could be true. It may
      also be specific to the Meta-World environments somehow.
    - Remove in-line imports for Meta-World and replace with try-catch imports at the
      top of each file.
    - Refactor metrics functionality so that there is a consistent way to distinguish
      between training metrics and evaluation metrics. At the moment there is so much
      conversion back and forth between two formats: (1) storing one version of the
      metric with flags that indicate whether that metric should be used for training
      and/or evaluation (2) storing one version of the metric for training and one
      version for evaluation. We should just make this consistent end-to-end.
- Additions:
    - Training options:
        - Choice of optimizer and optimizer hyperparameters
        - Learning rate schedules (ReduceLROnPlateau, cyclic LR)
    - Multi-task architecture support for environments with observations whose
      dimension is greater than 1
    - Add tests checking saved/loaded values for saving/loading training runs. This
      can't be done right now since the training function is so monolithic, we can't
      "look inside" and check the values of the local variabes of that function. Once
      this functionality is more modular we should add these tests. For now I just
      manually inspected the values after loading and created a baseline to compare
      metrics against. The same goes for saving/loading hyperparameter random search
      runs (the other types of hp search are reproducible when saving/loading, so
      there's no need for it there).
    - Allow for training on a sequence of configs by specifying a list of values in the
      config file instead of a single value. Just getting training to work like this
      will be easy, the more annoying part will be saving and loading over these
      compound runs.
    - Measure training metrics on each individual task during multi-task training,
      create a single plot with a subplot for each individual task.
    - Saving/loading for meta-learning runs.
    - Add option to meta-test by loading a meta-trained model.
    - Move data augmentation settings into config file
- Nice to have:
    - Compute standard deviation for success rate so that mean + stdev < 1 i.e. makes
      sense for a variable in the range [0, 1]. Easy way to do this is to assume that
      tan^(-1)(X) is normally distributed instead of X, compute mean + stdev and mean -
      stdev, then take tan of everything to put it back into interval [0, 1].
    - Time various parts of the training process, save along with metrics.
    - More robust way to save repository version along with results.
    - Catch KeyboardInterrupt during training to optionally stop training early while
      still saving out final metrics and plot.
    - When training starts and results are not being saved, notify user and give the
      option to turn saving on.
    - Make it so that when resuming training from a checkpoint, training happens exactly
      as it would had if training had never been stopped. Not sure if this is possible,
      the main obstacle being pickling/loading the environment state, and mimicking
      random decisions.
    - Monitor system resource utilization and recommend changes in config for maximum
      utilization, i.e. bigger batch sizes to increase GPU utilization, less processes
      if thrashing is present.
    - Output hyperparameter tables in performance plots.
    - Tests of statistical significance of results when comparing different methods.
    - History of saved experiments: Right now experiment history is only preserved
      through files written to "results/". It would be nice to access a list of
      experiments sorted by date and including name and some metrics.

RL training:
- Refactoring:
    - Refactor RolloutStorage data generators. Put tensors into a dict. Can we just use
      good indexing to construct the batches instead of looping and stacking?
    - Refactor naming in PPO implementation to clarify between episodes, rollouts, and
      trajectories
    - Rename PPO parameters (i.e. gamma -> discount)
    - Figure out the difference between our setup and other setups (such as multi-task
      PPO in the Meta-World paper) which use a linear feature baseline.
- Additions:
    - Test PPO for environments with action space Box((n1, n2))
    - Independent options for normalizing observations and rewards
    - Network architecture hyperparameters:
        - Parameter sharing between actor and critic (and separate recurrent layers)
        - Recurrent layers throughout network, different types of recurrent layers
        - Value function normalization
    - Options to collect data from multiple tasks. Cycle through tasks, randomly pick
      next, randomly pick next that is different from previous. One option should ensure
      that multi-process training is collecting data from tasks as uniformly as
      possible.
- Nice to have:
    - Rendering of episode videos
    - Determine whether or not policy has solved environment (N successes in a row,
      usually)
    - Deparallelization factor as an option. We include an option called
      `rollouts_per_process` that dictates how many rollouts each process should execute
      during a training step. Ideally, running 4 processes with 1 rollout per process
      would be the same as running 2 processes with 2 rollouts per process, but having
      the option to choose allows us to maximize the efficiency of whatever hardware we
      are running on. On my laptop, the limiting factor is RAM, so decreasing
      parallelization (i.e. increasing rollouts_per_process) is the way to go to avoid
      thrashing. The reason I haven't implemented this yet is because there isn't really
      a good answer to the following: should we reset the environment between
      consecutive rollouts for the same process? If the answer is yes, then we will
      never get past the first `rollout_length` steps of an episode, but if the answer
      is no, then running multiple rollouts per process is effectively the same as just
      increasing the rollout length. What we really want is to be able to reset the
      environment between rollouts from the same process, then restore the environment
      state of each rollout after the training step. However, this requires storing the
      environment state (don't wanna do that) and would greatly increase memory cost,
      which is the whole reason we want a deparallelization option in the first place.
      The increase in memory wouldn't actually be that bad, because even though the
      amount of memory needed for each process would increase by a factor of
      `rollouts_per_process`, that memory would be partitioned into
      `rollouts_per_process` pieces which each only need to exist in RAM one at a time,
      so I don't think we would see thrashing here. Still, storing and restoring the
      environment state is a pain and for now I'm just gonna increase the rollout
      length.

SL training:
- Refactoring:
    - Clean up Uncertainty/GradNorm/SLW junk in SLTrainer's initialization
    - Clean up PCGrad. Could separate into MTL trainer, or we could try implement this
      as a loss weighter, since PCGrad can technically be formulated as a loss weighting
      method.
    - Get evaluation to happen once per epoch automatically instead of requiring manual
      entry of evaluation_freq.
    - Clean up pre_loss_update. Just have a property within each loss weighter that
      determines whether weights are updated before or after loss computation.
    - Convert BN layers to test mode?
    - Fix memory usage issue. Commit dcdee654 was the last commit before we changed the
      evaluation format so that every evaluation step goes over the entire dataset. When
      we did that, the memory usage increased dramatically, requiring us to cut our
      batch size nearly in half. I still don't know what caused this, but fixing it
      would give us a ton of memory back.
- Additions:
    - MLP support for supervised learning.
- Nice to have:

Network architectures:
- Refactoring:
    - Remove redundant calls to "self.to(device)" in nested modules?
    - Add multitask capability to ConvNetwork
    - Merge redundant networks (Trunk + Backbone? Conv + MLP?)
    - Iron out inconsistency with specification of task output sizes for multi-task
      learning. Is the output size per-task or total? Seems to be inconsistent in our
      configuration for PCBA, MTRegression, and NYUv2.
- Additions:
    - Architecture hyperparameters:
        - Network initialization options
    - Convolutional network features:
        - Max pooling
        - Batch normalization
    - An option to turn on checks during inference (safe mode). We would check that each
      observation in a multi-task setting ends with a one-hot tensor of size
      `self.num_tasks`, and that task gradients from get_task_grads() from
      MultiTaskSplittingNetwork have the same zero padding structure before finding
      their difference, etc. This way we still have the option to check and be safe, but
      we don't have to waste computation time doing so if we don't want to. This also
      applies to checks of tensor structure in forward() of MultiTaskTrunkNetwork,
      get_loss() from PPOPolicy, and probably a bunch of other places.
- Nice to have:

Splitting networks:
- Refactoring:
    - Refactor MultiTaskSplittingNetwork.get_task_grad_diffs() to only compute pairwise
      differences between task gradients for tasks that share a copy. Right now it is
      computed for each pair of tasks at each region. We could also parallelize the
      computation across regions but I'm not quite sure how to do that.
    - In splitting networks, modify task gradient computation so that we don't perform a
      redundant backwards pass. Right now, we pass over each task loss, then zero
      everything out and pass over the sum of losses. We don't actually need to do that
      last backward pass, but we need to do some stuff to get the task losses without
      zeroing out gradients between each backward pass. Really we just need to take the
      successive differences of gradients, and we also need to change the format of the
      backward pass in general so that .backward() does get called on the entire loss if
      we aren't using splitting networks. Also, the problem is twice as bad as we
      originally thought: we are calling backwards on each task loss twice! Once in
      check_for_splits() for the actor and again for the critic. Fixing this
      inefficiency is gonna be messy.
    - Refactor MultiTaskSplittingNetwork.update_grad_stats() to get task flags in a way
      that is less brittle than checking whether the gradient vector is all zero. Maybe
      this is an edge case that we actually want to account for. We could probably get
      this info from `task_losses`.
- Additions:
    - Option to reset splitting statistics after a split is performed (this means we
      have to split regions with highest z-scores first)
    - Try training splitting networks by measuring gradient variance online, just
      with a smaller EMA alpha value. It might be that an EMA which moves too slowly
      will yield a variance which is larger than it should be, making the test for
      splitting not sensitive enough.
    - Support the inclusion of task index in input to splitting/meta-splitting networks.
      This is pretty messy, since we need to distinguish tasks from meta-training
      distribution and tasks from meta-testing distribution, but we also need to use the
      same layers between meta-training nad meta-testing. We can't just include the task
      index, because there may not be the same number of training/testing tasks. Even if
      there are, the network would think that an observation from testing task 1
      actually came from training task 1. If we try to use one-hot vectors whose size is
      the total number of training+testing tasks, then we end up in a situation where
      the weights used to transform the testing task inputs never get trained during
      meta-training, and then get frozen for meta-testing. The only solution I can think
      to this is to unfreeze specificially those weights during meta-testing, but this
      is not super clean and also goes against the spirit of modularity in splitting
      networks? Not sure.
- Nice to have:
    - In actor/critic network, add splitting for action_logstd. Right now each task just
      has a separate set of these parameters.
    - Allow weights/biases to be split independently

Experiment:
- Refactoring:
    - Extend Metrics object to hold metrics from multiple runs, with multiple methods. This
      will allow us to clean things up in places where we have a list of Metrics that all
      have redundant information.
    - Add some constraints to `experiment()` so that some options must remain fixed between
      different methods, such as trainer type and dataset.
- Additions:
    - Option to resume a previous experiment that only ran partially.
    - Option to add to an existing experiment. This way we could run an experiment with
      some baselines, then implement other baselines and just add in the results.
- Nice to have:

Report:
- Refactoring:
    - Use name_subs for plotting in `report()`.
- Additions:
    - Automatically bold results from best method when creating LaTeX table in `tabulate()`.
    - Allow more flexibility in settings for `tabulate()`. Settings can include list of
      methods to include, list of metrics to include, and settings or each method and
      metric: decimal precision, whether to include CI radius, display names, etc. Also, we
      can group the metrics and methods, then place table dividers accordingly.
- Nice to have:

Hyperparameter search:
- Refactoring:
    - Get rid of redundant settings inside and outside base_train_config for
      hyperparameter search, i.e. seed, save name, etc
    - Get rid of repeated code in different search strategies
    - Clean up redundant options between "base_train_config" and "hp_config"
    - Change hyperparameter search to allow non-unique leaf node names in config. After
      allowing separate architecture configs for actor and critic networks, we have a
      bunch of parameters with non-unique names. What this means is that parameters with
      the same name will be tied together during the search process. To fix this, we
      need to change the way that we specify search spaces for parameters. Right now
      each search space is tied to a parameter just by its name, which is why we needed
      unique names, but what we really need is for each search space to be tied to the
      path through the config tree to the corresponding leaf. To do this, we should just
      specify the search spaces within the base train config dictionary. Instead of
      having individual values within this config, we just have a dict of values that
      works the same way as the dictionaries to modify configs in `experiment` config
      files.
- Additions:
    - An actual good random search: Check Bengio et al. (2014ish) for a competitive
      random hyperparameter search technique
    - Another hp search strategy: IC grid search, but which parameter to vary is chosen
      automatically. Each parameter is varied once to get an initial tuning for each
      one, then the next parameter to tune is sampled from a distribution in which the
      probability that a parameter gets chosen is equal to some normalized estimate of
      the potential gain from tuning that parameter. A natural choice is to use the
      standard deviation of the rewards from the previous tuning as this estimate. Once
      a parameter is chosen, we compute an interval to vary that parameter over based
      off of the previous tuned value of that parameter and the interval type
      (arithmetic, geometric).  Not sure how this will work with discrete variables but
      we can at least use this for continuous parameters.
    - Add option to use meta_train() instead of train().
    - Support for new Metric format. We should be using Metric.best instead of
      Metric.maximum, since some metrics should be minimized instead of maximized.
- Nice to have:
    - Plots to show progression of various hyperparameters during hyperparameter search
    - Include name of search parameter in name of results file for IC grid search
    - Make it so that tune runs are consistent with train runs. This means if we run the
      same config through "train" as with one step of "tune", they will create the same
      results. This will likely involve separating the random seeds used in each.

Tests:
- Refactoring:
    - Modify tests so that we don't have to save/load from/to disk during tests, which
      is possible now that we return checkpoints from train().
    - In tests/networks/test_trunk.py, test_backward() asserts that the generated
      observation batch contains data from all tasks, but the obs batch isn't explicitly
      constructed this way. It happens to pass by luck but we should enforce this so it
      doesn't randomly break if seed or something else changes.
    - Make tests faster by decreasing training lengths and replacing complex
      environments (particularly MetaWorld) with simpler ones.
    - Replace unnecessary training tests with tests on easy benchmarks (CIFAR, MNIST)
      that only check for sufficiently high accuracy.
- Additions:
    - Tests for solving simple openai gym environments in a small amount of time
    - Add tests for `experiment()` to check that results are consistent with `train()`.
    - Tests for loss weighters (remove training tests).
- Nice to have:

Style:
- Refactoring:
    - Change keywords for string options to name of corresponding classes. Example,
      architecture types can just be MLPNetwork, ConvNetwork, etc.
    - Remove as many default arguments as possible.
    - Clean up imports so that we import from a subpackage's __init__.py instead of from
      each file in that subpackage.
    - Convert config into an object or just store config values as members in the
      relevant class, so that we can get rid of magic strings everywhere.
    - Fix security issues with `eval()`. Anytime `eval()` is used, we need to check that
      the input is a member of a fixed whitelist.
    - Rename MultiTaskTrunkNetwork to TrunkNetwork, MultiTaskSplittingNetwork to
      SplittingNetwork
- Additions:
- Nice to have:
